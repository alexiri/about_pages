{
  "title": {
    "en": "The Grid: Software, middleware, hardware",
    "fr": "La Grille : les logiciels, les intergiciels, le mat\u00e9riel informatique"
  },

  "hook": {
    "en": "Find out how physics software, middleware, hardware and networking all contribute to the Worldwide LHC Computing Grid",
    "fr": "D\u00e9couvrez comment les logiciels de physique, les intergiciels, le mat\u00e9riel informatique et les r\u00e9seaux contribuent \u00e0 la Grille"
  },

  "body": {
    "en": "<p>The four main component layers of the <a href=\"https://home.cern/science/computing/worldwide-lhc-computing-grid\">Worldwide LHC Computing Grid </a>(WLCG) are physics software, middleware, hardware and networking.</p><h3>Physics software</h3><p>WLCG computer centres are made up of multi-petabyte storage systems and computing clusters with thousands of nodes connected by high-speed networks. They need software tools that go beyond what is commercially available to satisfy the changing demands of the high-energy-physics community.</p><p>The physics software on the Grid includes programs such as <a href=\"https://root.cern.ch//\">ROOT</a>, a set of object-oriented core libraries used by all the LHC <a href=\"https://home.cern/science/experiments\">experiments</a>; <a href=\"http://pool.cern.ch/\">POOL</a>, a framework that provides storage for event data; and other software for modelling the generation, propagation and interactions of elementary particles. Grid projects supply much of the software that manages distribution and access, as well as job submission, user authentication and authorization. They also supply software known collectively as \"middleware\".</p><h3>Middleware</h3><p>Although the Grid depends on the computer and communications networks of the underlying internet, novel software allows users to access computers distributed across the network. This software is called \"middleware\" because it sits between the operating systems of the computers and the physics-applications software that can solves a user's particular problem.</p><p>The most important middleware stacks in the WLCG are the European Middleware Initiative, which combines key middleware providers ARC, <a href=\"http://glite.web.cern.ch/glite/\">gLite</a>, <a href=\"https://www.unicore.eu/\">UNICORE</a> and <a href=\"https://www.dcache.org/\">dCache</a>; <a href=\"https://www.globus.org/\">Globus Toolkit</a> developed by the Globus Alliance; OMII from the Open Middleware Infrastructure Institute; and <a href=\"http://vdt.cs.wisc.edu/\">Virtual Data Toolkit</a>.</p><h3>Hardware</h3><p>Each Grid centre manages a large collection of computers and storage systems. Installing and regularly upgrading the necessary software manually is labour intensive, so management systems such as <a href=\"http://quattor.sourceforge.net/\">Quattor </a>(developed at CERN) automate these services. They ensure that the correct software is installed from the operating system all the way to the experiment-specific physics libraries, and make this information available to the overall Grid scheduling system, which decides which centres are available to run a particular job.</p><p>Each of the 11 Tier 1 centres also maintains disk and tape servers, which need to be upgraded regularly. These centres use specialized storage tools \u2013 such as the  <a href=\"https://www.dcache.org/\">dCache system</a> developed at the <a href=\"http://www.desy.de/\"><em>Deutsches Elektronen Synchrotron</em></a> (DESY) laboratory in Germany, the <a href=\"http://www-ccf.fnal.gov/enstore/\">ENSTORE</a> system at <a href=\"http://www.fnal.gov/\">Fermilab</a> in the US or the <a href=\"http://castor.web.cern.ch/\">CERN advanced storage system</a> (CASTOR) developed at CERN \u2013 to allow access to data for simulation and analysis independent of the medium (tape or disk) that the information is stored on.</p><h3>Networking</h3><p>The Grid file-transfer service, developed by the Enabling Grids for E-science projects, manages the exchange of information between WLCG centres. The file-transfer service has been tailored to support the special needs of grid computing, including authentication and confidentiality features, reliability and fault tolerance, and third-party and partial-file transfer.</p><p>Optical-fibre links working at 10 gigabits per second connect CERN to each of the Tier 1 centres around the world. This dedicated high-bandwidth network is called the LHC Optical Private Network (LHCOPN).</p><h4>Resources</h4><ul><li><a href=\"https://netstat.cern.ch/monitoring/network-statistics/visual/?p=ge\">Google Earth overlay</a>: current network statistics and traffic details</li><li><a href=\"https://netstat.cern.ch/monitoring/network-statistics/weathermap/?map=LHCOPN\">LHCOPN \"weathermap\"</a>: real-time traffic status between CERN and other Tier 1 sites</li></ul><h4>Maps</h4><ul><li>All WLCG sites</li><li><a href=\"http://dashb-earth.cern.ch/dashboard/doc/guides/service-monitor-gearth/html/user/index.html?kmlid=all\">Google Earth live dynamic map</a>: all WLCG live grid sites, links, data transfer and job activity over the last few minutes</li><li><a href=\"https://netstat.cern.ch/monitoring/network-statistics/visual/?p=ge\">Google Earth network architecture layer</a>: sites and traffic statistics</li></ul>",

    "fr": "<p>Les quatre principaux \u00e9l\u00e9ments de la Grille de calcul mondiale pour le LHC (WLCG) sont les logiciels de physique, les intergiciels, le mat\u00e9riel informatique et les r\u00e9seaux.</p><h2>Les logiciels de physique</h2><p>Les centres de calcul du WLCG sont compos\u00e9s de syst\u00e8mes de stockage de plusieurs p\u00e9taoctets et d'ordinateurs en grappe, avec des milliers de n\u0153uds connect\u00e9s par des r\u00e9seaux haut d\u00e9bit. Afin de r\u00e9pondre aux besoins \u00e9volutifs de la communaut\u00e9 de la physique des hautes \u00e9nergies, ces centres de calcul requi\u00e8rent des outils logiciels plus sophistiqu\u00e9s que ceux disponibles dans le commerce.</p><p>Parmi les logiciels de physique qu\u2019utilise la Grille figurent des programmes tels que <a href=\"https://root.cern.ch/\">ROOT</a>, une s\u00e9rie de biblioth\u00e8ques centrales orient\u00e9es objet et utilis\u00e9es par toutes les <a href=\"https://home.cern/science/experiments\">exp\u00e9riences</a> LHC ; <a href=\"http://pool.cern.ch/\">POOL</a>, une structure qui permet de stocker les donn\u00e9es des \u00e9v\u00e9nements ; et d'autres logiciels permettant de mod\u00e9liser la production, la propagation et les interactions des particules \u00e9l\u00e9mentaires. Les projets de la Grille fournissent une grande partie des logiciels qui g\u00e8rent la distribution et l\u2019acc\u00e8s, l\u2019envoi des calculs, ainsi que l\u2019authentification et l\u2019autorisation des utilisateurs.  Ils fournissent \u00e9galement des logiciels sous le nom d\u2019\u00ab intergiciels \u00bb.</p><h2>Les intergiciels</h2><p>Bien que la Grille d\u00e9pende des r\u00e9seaux informatiques et de communication de l\u2019internet sous-jacent, des logiciels novateurs permettent aux utilisateurs d\u2019acc\u00e9der aux ordinateurs r\u00e9partis dans le monde entier. Ces logiciels sont appel\u00e9s \u00ab intergiciels \u00bb, car ils se situent entre les syst\u00e8mes d\u2019exploitation des ordinateurs et les logiciels d\u2019applications de physique capables de r\u00e9soudre les probl\u00e8mes sp\u00e9cifiques des utilisateurs.</p><p>Les piles d\u2019intergiciels les plus importantes du WLCG sont l\u2019EMI (European Middleware Initiative), qui regroupe d\u2019importants fournisseurs  d\u2019intergiciels, tels que ARC, <a href=\"http://glite.web.cern.ch/glite/\">gLite</a>, <a href=\"https://www.unicore.eu/\">UNICORE</a> et <a href=\"https://www.dcache.org/\">dCache</a>; <a href=\"https://www.globus.org/\">Globus Toolkit</a>, d\u00e9velopp\u00e9 par Globus Alliance ; OMII, de l\u2019Open Middleware Infrastructure Institute ; et <a href=\"http://vdt.cs.wisc.edu/\">Virtual Data Toolkit</a>.</p><h2>Le mat\u00e9riel informatique</h2><p>Chaque centre de calcul g\u00e8re de nombreux ordinateurs et syst\u00e8mes de stockage. L\u2019installation et la mise \u00e0 jour manuelle et r\u00e9guli\u00e8re des logiciels est un travail intensif, raison pour laquelle des syst\u00e8mes de gestion, tels que Quattor (d\u00e9velopp\u00e9 au CERN), r\u00e9alisent ces op\u00e9rations automatiquement. Ces syst\u00e8mes font en sorte que les logiciels ad\u00e9quats soient install\u00e9s, depuis le syst\u00e8me d\u2019exploitation jusqu'aux biblioth\u00e8ques propres \u00e0 chaque exp\u00e9rience, et que l\u2019information soit disponible pour l\u2019ensemble du syst\u00e8me d\u2019ordonnancement de la Grille, qui attribue les calculs aux centres disponibles.</p><p>Les 11 centres de niveau 1 ont \u00e9galement en charge la maintenance des serveurs de stockage sur disque et sur bande, qui doivent \u00eatre mis \u00e0 niveau r\u00e9guli\u00e8rement. Ces centres utilisent des outils de stockage sp\u00e9cialis\u00e9s, tels que le syst\u00e8me <a href=\"https://www.dcache.org/\">dCache</a>, d\u00e9velopp\u00e9 au laboratoire <a href=\"http://www.desy.de/\">DESY</a> (Deutsches Elektronen Synchrotron) en Allemagne, le syst\u00e8me <a href=\"http://www-ccf.fnal.gov/enstore/\">ENSTORE</a> au <a href=\"http://www.fnal.gov/\">Fermilab</a> (\u00c9tats-Unis) ou le syst\u00e8me <a href=\"http://castor.web.cern.ch/\">CASTOR</a> (CERN advanced storage system), d\u00e9velopp\u00e9 au CERN, afin de permettre l\u2019acc\u00e8s aux donn\u00e9es, pour la simulation et l\u2019analyse, quel que soit le support (disque ou bande) sur lequel les donn\u00e9es sont stock\u00e9es.</p><h2>Les r\u00e9seaux</h2><p>Le service de transfert de fichiers de la Grille, d\u00e9velopp\u00e9 par les projets Enabling Grids for E-science, g\u00e8re l\u2019\u00e9change d\u2019informations entre les diff\u00e9rents centres de la Grille. Le service de transfert de fichiers a \u00e9t\u00e9 con\u00e7u pour r\u00e9pondre aux besoins particuliers du calcul sur grille, notamment l\u2019authentification et la confidentialit\u00e9, la fiabilit\u00e9 et la tol\u00e9rance aux pannes, et le transfert \u00e0 un tiers ou le transfert partiel de fichiers.</p><p>Des liaisons de fibres optiques fonctionnant \u00e0 un d\u00e9bit de 10 gigabits par seconde relient le CERN aux centres de niveau 1 du monde entier. Ce r\u00e9seau large bande d\u00e9di\u00e9 est le R\u00e9seau optique priv\u00e9 du LHC (LHCOPN, LHC Optical Private Network).</p><h3>Ressources</h3><ul><li><a href=\"https://netstat.cern.ch/monitoring/network-statistics/visual/?p=ge\">Couverture par Google Earth</a> : statistiques du r\u00e9seau et d\u00e9tails du trafic actualis\u00e9s</li><li><a href=\"https://netstat.cern.ch/monitoring/network-statistics/weathermap/?map=LHCOPN\">Situation du LHCOPN</a> : \u00e9tat du trafic en temps r\u00e9el entre le CERN et les sites de niveau 1</li><li>Architecture de r\u00e9seau de haut niveau pour le LHC</li></ul><h3>Cartes</h3><ul><li>Tous les sites du WLCG</li><li><a href=\"http://dashb-earth.cern.ch/dashboard/doc/guides/service-monitor-gearth/html/user/index.html?kmlid=all\">Carte dynamique et en temps r\u00e9el de Google Earth</a>: tous les sites de la Grille, les liens, le tranfert de donn\u00e9es et l\u2019activit\u00e9 de calcul, minute par minute</li><li><a href=\"https://netstat.cern.ch/monitoring/network-statistics/visual/?p=ge\">Couche architecture r\u00e9seau Google Earth</a> : sites et statistiques du trafic</li></ul><p> </p>"
  },

  "image_url": "https://home.cern/sites/home.web.cern.ch/files/image/computing_page/2013/01/storage-tape_0.jpg",
  "oda_tags": ["computing"],
  "id": "science/computing/grid-software-middleware-hardware"
}
